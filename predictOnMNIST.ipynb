{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import sys\n",
    "import os\n",
    "import copy\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "if sys.version_info[0] == 2:\n",
    "    from urllib import urlretrieve\n",
    "else:\n",
    "    from urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download(filename, source='http://yann.lecun.com/exdb/mnist/'):\n",
    "    print(\"Downloading %s\" % filename)\n",
    "    urlretrieve(source + filename, filename)\n",
    "\n",
    "def load_mnist_images(filename):\n",
    "    if not os.path.exists(filename):\n",
    "        download(filename)\n",
    "    # Read the inputs in Yann LeCun's binary format.\n",
    "    with gzip.open(filename, 'rb') as f:\n",
    "        data = np.frombuffer(f.read(), np.uint8, offset=16)\n",
    "    data = data.reshape(-1,784)\n",
    "    return data / np.float32(256)\n",
    "\n",
    "def load_mnist_labels(filename):\n",
    "    if not os.path.exists(filename):\n",
    "        download(filename)\n",
    "    with gzip.open(filename, 'rb') as f:\n",
    "        data = np.frombuffer(f.read(), np.uint8, offset=8)\n",
    "        #data2 = np.zeros( (len(data),10), dtype=np.float32 )\n",
    "        #for i in range(len(data)):\n",
    "        #    data2[i][ data[i] ] = 1.0\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading train-images-idx3-ubyte.gz\n",
      "Downloading train-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "train_data = load_mnist_images('train-images-idx3-ubyte.gz')\n",
    "train_labels = load_mnist_labels('train-labels-idx1-ubyte.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "class layer(): # layer class\n",
    "    def __init__(self, w, b, size):\n",
    "        self.w = w # w matrix (j'th row stores a row vector of weights of j'th neuron)\n",
    "        self.b = b # b vector (b[j] is the bias term of the j'th neuron in this layer)\n",
    "        self.size = size # number of neurons in this layer\n",
    "        self.values = np.empty # vector of values\n",
    "        self.output = np.empty # vector of output values\n",
    "        self.derivatives = np.empty # vector of derivatives\n",
    "        \n",
    "class ffnn(): # neural network class\n",
    "    def __init__(self, h_layers, learning_rate = 0.1, batch_size = 0.1, targetAccuracy = 0.9):\n",
    "        self.h_layers = h_layers\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size # share of the dataset used to update params\n",
    "        self.layers = np.empty # layers are later added by the 'construct' method\n",
    "        self.targetAccuracy = targetAccuracy # when this accuracy (plus sufficient convergence) is achieved, training is halted\n",
    "        \n",
    "    def __construct(self, d, k = 1): # private constructor method to create layers and initialize params\n",
    "        # d is number of features\n",
    "        # k is number of classes in classification\n",
    "        \n",
    "        self.layers = np.empty(len(self.h_layers) + 2, layer) # initialize array of layers to empty array of dimension h + 2 (h is number of hidden layers)\n",
    "        self.layers[0] = layer(np.empty, np.zeros(d), d) # initialize input layer with random b vector\n",
    "\n",
    "        for h in range(len(self.h_layers)): # create hidden layers\n",
    "            self.layers[h+1] = layer(np.random.rand(self.h_layers[h], self.layers[h].size) - 0.5, np.random.rand(self.h_layers[h]) - 0.5, self.h_layers[h])\n",
    "        \n",
    "        # output layer\n",
    "        self.layers[-1] = layer(np.random.rand(k, self.layers[-2].size) - 0.5, np.random.rand(k) - 0.5, k)\n",
    "    \n",
    "    def __randomParams(self): # private method for setting new random weight matrix for all layers\n",
    "        for i in range(1, len(self.h_layers)+2):\n",
    "            self.layers[i].w = np.random.rand(self.layers[i].w.shape[0], self.layers[i].w.shape[1])-0.5\n",
    "    \n",
    "    def train(self, x, y):\n",
    "        iters = 0\n",
    "        eta = self.learning_rate\n",
    "        self.__construct(x.shape[1], len(np.unique(y))) # call internal constructor method\n",
    "        \n",
    "        data = np.concatenate((x, y), 1) # merge x and y data such that the data/labels correspond\n",
    "        batchSize = int(self.batch_size * x.shape[0])\n",
    "        \n",
    "        while(not self.__earlyStopping(x,y,iters)):\n",
    "            # randomly select batch to train on\n",
    "            index = np.random.randint(x.shape[0], size = batchSize) # create random index set to sample\n",
    "            batch = data[index,:] # create random sample from the data\n",
    "            \n",
    "            for sample in range(batchSize): # iterate through all samples in the batch\n",
    "                point = batch[sample, :-1]\n",
    "                label = batch[sample, -1]\n",
    "                self.__forwardPass(point, label) # call internal prediction method\n",
    "                \n",
    "                # compute derivative with respect to nodes in last layer\n",
    "                self.layers[-1].derivatives = (self.layers[-1].values - self.__oneHot(label))/batchSize\n",
    "                # change w and b\n",
    "                self.layers[-1].w -= np.dot(np.reshape(self.layers[-1].derivatives, (self.layers[-1].size, 1)), np.reshape(self.layers[-2].values.T, (1,self.layers[-2].size)))*eta\n",
    "                self.layers[-1].b -= self.layers[-1].derivatives * eta\n",
    "    \n",
    "                for i in range(len(self.layers)-2, 0, -1):\n",
    "                    self.layers[i].derivatives = np.dot(self.layers[i+1].w.T, self.layers[i+1].derivatives)\n",
    "                    self.layers[i].b -= self.layers[i].derivatives * eta\n",
    "                    self.layers[i].w -= np.dot(np.reshape(self.layers[i].derivatives, (self.layers[i].size, 1)), np.reshape(self.layers[i-1].values.T, (1,self.layers[i-1].size)))*eta\n",
    "                        \n",
    "            iters += 1\n",
    "            print(\"iterations: \", iters)\n",
    "                    \n",
    "    def __oneHot(self, x):\n",
    "        vect = np.zeros(self.layers[-1].size)\n",
    "        vect[int(x)] = 1\n",
    "        return vect\n",
    "    \n",
    "    def __forwardPass(self, x, y): # accepts x vector and y label (forward pass for one label)\n",
    "        self.layers[0].values = x\n",
    "        for i in range(1, self.layers.size): # compute layer values as matmuls\n",
    "            layer = self.layers[i]\n",
    "            layer.values = np.dot(layer.w, self.layers[i-1].values) + layer.b\n",
    "            if (i != (self.layers.size - 1)): layer.values[layer.values < 0] = 0 # apply ReLU\n",
    "        self.layers[-1].values = self.__softmax(self.layers[-1].values)\n",
    "        return np.argmax(self.layers[-1].values) # predict the maximum value in the output layer\n",
    "    \n",
    "    def __softmax(self, x):\n",
    "        maxi = np.max(x) # normalize the values to prevent overflows\n",
    "        sumAll = sum(np.exp(x-maxi))\n",
    "        return np.exp(x-maxi)/sumAll\n",
    "    \n",
    "    def predict(self, x, y): # public prediction method\n",
    "        preds = np.empty(x.shape[0],int)\n",
    "        for i in range(x.shape[0]):\n",
    "            preds[i] = self.__forwardPass(x[i], y[i])\n",
    "        acc = self.__accuracy(preds, y)\n",
    "        return preds, acc\n",
    "            \n",
    "    def __earlyStopping(self, x, y, iteration): # decides when to stop the training\n",
    "        global oldAccuracy\n",
    "        if (iteration % 10 != 0): return False \n",
    "        if iteration == 0: # if it's the first iteration, skip everything here right away\n",
    "            last_error = 999999999999999 # error of last iteration to compare if it changed (convergence check)\n",
    "            oldAccuracy = 0.3 # set accuracy to 30%\n",
    "            return False\n",
    "        else: # check only every 10'th iteration\n",
    "            data = np.concatenate((x, y), 1) # merge x and y so that they correspond (to sample from it)\n",
    "            index = np.random.randint(x.shape[0], size = int(x.shape[0]*0.1)) # create random index set to sample\n",
    "            sample = data[index,:] # create random sample from the data\n",
    "            predictions = self.predict(sample[:,:-1], sample[:,-1])[0] # predict on the sample\n",
    "            accuracy = self.__accuracy(predictions, sample[:, -1])\n",
    "            print(\"current accuracy: \", accuracy)\n",
    "            if ((accuracy < oldAccuracy) & (accuracy < 0.3)): # if accuracy decreased and is less than 30%, put new random parameters\n",
    "                print(\"new random params\")\n",
    "                self.__randomParams()\n",
    "                oldAccuracy = 0.3\n",
    "                return False\n",
    "            # if accuracy barely changes and achieved target accuracy, stop training\n",
    "            if ((abs(accuracy - oldAccuracy) < 0.1) & (accuracy > self.targetAccuracy)): return True\n",
    "            oldAccuracy = accuracy\n",
    "            return False\n",
    "    \n",
    "    def __accuracy(self, preds, y):\n",
    "        if (y.size == 0): raise Exception(\"empty sample\")\n",
    "        errors = 0\n",
    "        for i in range (y.size):\n",
    "            if (preds[i] != int(y[i])): errors += 1\n",
    "        return 1 - errors/y.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, testX, trainY, testY = train_test_split(train_data, train_labels, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterations:  1\n",
      "iterations:  2\n",
      "iterations:  3\n",
      "iterations:  4\n",
      "iterations:  5\n",
      "iterations:  6\n",
      "iterations:  7\n",
      "iterations:  8\n",
      "iterations:  9\n",
      "iterations:  10\n",
      "current accuracy:  0.46238095238095234\n",
      "iterations:  11\n",
      "iterations:  12\n",
      "iterations:  13\n",
      "iterations:  14\n",
      "iterations:  15\n",
      "iterations:  16\n",
      "iterations:  17\n",
      "iterations:  18\n",
      "iterations:  19\n",
      "iterations:  20\n",
      "current accuracy:  0.5938095238095238\n",
      "iterations:  21\n",
      "iterations:  22\n",
      "iterations:  23\n",
      "iterations:  24\n",
      "iterations:  25\n",
      "iterations:  26\n",
      "iterations:  27\n",
      "iterations:  28\n",
      "iterations:  29\n",
      "iterations:  30\n",
      "current accuracy:  0.6490476190476191\n",
      "iterations:  31\n",
      "iterations:  32\n",
      "iterations:  33\n",
      "iterations:  34\n",
      "iterations:  35\n",
      "iterations:  36\n",
      "iterations:  37\n",
      "iterations:  38\n",
      "iterations:  39\n",
      "iterations:  40\n",
      "current accuracy:  0.7052380952380952\n",
      "iterations:  41\n",
      "iterations:  42\n",
      "iterations:  43\n",
      "iterations:  44\n",
      "iterations:  45\n",
      "iterations:  46\n",
      "iterations:  47\n",
      "iterations:  48\n",
      "iterations:  49\n",
      "iterations:  50\n",
      "current accuracy:  0.7214285714285714\n",
      "iterations:  51\n",
      "iterations:  52\n",
      "iterations:  53\n",
      "iterations:  54\n",
      "iterations:  55\n",
      "iterations:  56\n",
      "iterations:  57\n",
      "iterations:  58\n",
      "iterations:  59\n",
      "iterations:  60\n",
      "current accuracy:  0.7609523809523809\n",
      "iterations:  61\n",
      "iterations:  62\n",
      "iterations:  63\n",
      "iterations:  64\n",
      "iterations:  65\n",
      "iterations:  66\n",
      "iterations:  67\n",
      "iterations:  68\n",
      "iterations:  69\n",
      "iterations:  70\n",
      "current accuracy:  0.7697619047619048\n",
      "iterations:  71\n",
      "iterations:  72\n",
      "iterations:  73\n",
      "iterations:  74\n",
      "iterations:  75\n",
      "iterations:  76\n",
      "iterations:  77\n",
      "iterations:  78\n",
      "iterations:  79\n",
      "iterations:  80\n",
      "current accuracy:  0.78\n",
      "iterations:  81\n",
      "iterations:  82\n",
      "iterations:  83\n",
      "iterations:  84\n",
      "iterations:  85\n",
      "iterations:  86\n",
      "iterations:  87\n",
      "iterations:  88\n",
      "iterations:  89\n",
      "iterations:  90\n",
      "current accuracy:  0.7776190476190477\n",
      "iterations:  91\n",
      "iterations:  92\n",
      "iterations:  93\n",
      "iterations:  94\n",
      "iterations:  95\n",
      "iterations:  96\n",
      "iterations:  97\n",
      "iterations:  98\n",
      "iterations:  99\n",
      "iterations:  100\n",
      "current accuracy:  0.8042857142857143\n",
      "iterations:  101\n",
      "iterations:  102\n",
      "iterations:  103\n",
      "iterations:  104\n",
      "iterations:  105\n",
      "iterations:  106\n",
      "iterations:  107\n",
      "iterations:  108\n",
      "iterations:  109\n",
      "iterations:  110\n",
      "current accuracy:  0.7938095238095237\n",
      "iterations:  111\n",
      "iterations:  112\n",
      "iterations:  113\n",
      "iterations:  114\n",
      "iterations:  115\n",
      "iterations:  116\n",
      "iterations:  117\n",
      "iterations:  118\n",
      "iterations:  119\n",
      "iterations:  120\n",
      "current accuracy:  0.8121428571428572\n",
      "iterations:  121\n",
      "iterations:  122\n",
      "iterations:  123\n",
      "iterations:  124\n",
      "iterations:  125\n",
      "iterations:  126\n",
      "iterations:  127\n",
      "iterations:  128\n",
      "iterations:  129\n",
      "iterations:  130\n",
      "current accuracy:  0.8095238095238095\n",
      "iterations:  131\n",
      "iterations:  132\n",
      "iterations:  133\n",
      "iterations:  134\n",
      "iterations:  135\n",
      "iterations:  136\n",
      "iterations:  137\n",
      "iterations:  138\n",
      "iterations:  139\n",
      "iterations:  140\n",
      "current accuracy:  0.8097619047619048\n",
      "iterations:  141\n",
      "iterations:  142\n",
      "iterations:  143\n",
      "iterations:  144\n",
      "iterations:  145\n",
      "iterations:  146\n",
      "iterations:  147\n",
      "iterations:  148\n",
      "iterations:  149\n",
      "iterations:  150\n",
      "current accuracy:  0.8180952380952381\n",
      "iterations:  151\n",
      "iterations:  152\n",
      "iterations:  153\n",
      "iterations:  154\n",
      "iterations:  155\n",
      "iterations:  156\n",
      "iterations:  157\n",
      "iterations:  158\n",
      "iterations:  159\n",
      "iterations:  160\n",
      "current accuracy:  0.8197619047619048\n",
      "iterations:  161\n",
      "iterations:  162\n",
      "iterations:  163\n",
      "iterations:  164\n",
      "iterations:  165\n",
      "iterations:  166\n",
      "iterations:  167\n",
      "iterations:  168\n",
      "iterations:  169\n",
      "iterations:  170\n",
      "current accuracy:  0.825952380952381\n",
      "iterations:  171\n",
      "iterations:  172\n",
      "iterations:  173\n",
      "iterations:  174\n",
      "iterations:  175\n",
      "iterations:  176\n",
      "iterations:  177\n",
      "iterations:  178\n",
      "iterations:  179\n",
      "iterations:  180\n",
      "current accuracy:  0.8285714285714285\n",
      "iterations:  181\n",
      "iterations:  182\n",
      "iterations:  183\n",
      "iterations:  184\n",
      "iterations:  185\n",
      "iterations:  186\n",
      "iterations:  187\n",
      "iterations:  188\n",
      "iterations:  189\n",
      "iterations:  190\n",
      "current accuracy:  0.8388095238095238\n",
      "iterations:  191\n",
      "iterations:  192\n",
      "iterations:  193\n",
      "iterations:  194\n",
      "iterations:  195\n",
      "iterations:  196\n",
      "iterations:  197\n",
      "iterations:  198\n",
      "iterations:  199\n",
      "iterations:  200\n",
      "current accuracy:  0.84\n",
      "iterations:  201\n",
      "iterations:  202\n",
      "iterations:  203\n",
      "iterations:  204\n",
      "iterations:  205\n",
      "iterations:  206\n",
      "iterations:  207\n",
      "iterations:  208\n",
      "iterations:  209\n",
      "iterations:  210\n",
      "current accuracy:  0.8414285714285714\n",
      "iterations:  211\n",
      "iterations:  212\n",
      "iterations:  213\n",
      "iterations:  214\n",
      "iterations:  215\n",
      "iterations:  216\n",
      "iterations:  217\n",
      "iterations:  218\n",
      "iterations:  219\n",
      "iterations:  220\n",
      "current accuracy:  0.8483333333333334\n",
      "iterations:  221\n",
      "iterations:  222\n",
      "iterations:  223\n",
      "iterations:  224\n",
      "iterations:  225\n",
      "iterations:  226\n",
      "iterations:  227\n",
      "iterations:  228\n",
      "iterations:  229\n",
      "iterations:  230\n",
      "current accuracy:  0.8388095238095238\n",
      "iterations:  231\n",
      "iterations:  232\n",
      "iterations:  233\n",
      "iterations:  234\n",
      "iterations:  235\n",
      "iterations:  236\n",
      "iterations:  237\n",
      "iterations:  238\n",
      "iterations:  239\n",
      "iterations:  240\n",
      "current accuracy:  0.8504761904761905\n",
      "iterations:  241\n",
      "iterations:  242\n",
      "iterations:  243\n",
      "iterations:  244\n",
      "iterations:  245\n",
      "iterations:  246\n",
      "iterations:  247\n",
      "iterations:  248\n",
      "iterations:  249\n",
      "iterations:  250\n",
      "current accuracy:  0.8485714285714285\n",
      "iterations:  251\n",
      "iterations:  252\n",
      "iterations:  253\n",
      "iterations:  254\n",
      "iterations:  255\n",
      "iterations:  256\n",
      "iterations:  257\n",
      "iterations:  258\n",
      "iterations:  259\n",
      "iterations:  260\n",
      "current accuracy:  0.8409523809523809\n",
      "iterations:  261\n",
      "iterations:  262\n",
      "iterations:  263\n",
      "iterations:  264\n",
      "iterations:  265\n",
      "iterations:  266\n",
      "iterations:  267\n",
      "iterations:  268\n",
      "iterations:  269\n",
      "iterations:  270\n",
      "current accuracy:  0.8516666666666667\n",
      "iterations:  271\n",
      "iterations:  272\n",
      "iterations:  273\n",
      "iterations:  274\n",
      "iterations:  275\n",
      "iterations:  276\n",
      "iterations:  277\n",
      "iterations:  278\n",
      "iterations:  279\n",
      "iterations:  280\n",
      "current accuracy:  0.8483333333333334\n",
      "iterations:  281\n",
      "iterations:  282\n",
      "iterations:  283\n",
      "iterations:  284\n",
      "iterations:  285\n",
      "iterations:  286\n",
      "iterations:  287\n",
      "iterations:  288\n",
      "iterations:  289\n",
      "iterations:  290\n",
      "current accuracy:  0.854047619047619\n",
      "iterations:  291\n",
      "iterations:  292\n",
      "iterations:  293\n",
      "iterations:  294\n",
      "iterations:  295\n",
      "iterations:  296\n",
      "iterations:  297\n",
      "iterations:  298\n",
      "iterations:  299\n",
      "iterations:  300\n",
      "current accuracy:  0.8638095238095238\n",
      "iterations:  301\n",
      "iterations:  302\n",
      "iterations:  303\n",
      "iterations:  304\n",
      "iterations:  305\n",
      "iterations:  306\n",
      "iterations:  307\n",
      "iterations:  308\n",
      "iterations:  309\n",
      "iterations:  310\n",
      "current accuracy:  0.8592857142857143\n",
      "iterations:  311\n",
      "iterations:  312\n",
      "iterations:  313\n",
      "iterations:  314\n",
      "iterations:  315\n",
      "iterations:  316\n",
      "iterations:  317\n",
      "iterations:  318\n",
      "iterations:  319\n",
      "iterations:  320\n",
      "current accuracy:  0.8595238095238096\n",
      "iterations:  321\n",
      "iterations:  322\n",
      "iterations:  323\n",
      "iterations:  324\n",
      "iterations:  325\n",
      "iterations:  326\n",
      "iterations:  327\n",
      "iterations:  328\n",
      "iterations:  329\n",
      "iterations:  330\n",
      "current accuracy:  0.855952380952381\n",
      "iterations:  331\n",
      "iterations:  332\n",
      "iterations:  333\n",
      "iterations:  334\n",
      "iterations:  335\n",
      "iterations:  336\n",
      "iterations:  337\n",
      "iterations:  338\n",
      "iterations:  339\n",
      "iterations:  340\n",
      "current accuracy:  0.8628571428571429\n",
      "iterations:  341\n",
      "iterations:  342\n",
      "iterations:  343\n",
      "iterations:  344\n",
      "iterations:  345\n",
      "iterations:  346\n",
      "iterations:  347\n",
      "iterations:  348\n",
      "iterations:  349\n",
      "iterations:  350\n",
      "current accuracy:  0.8588095238095238\n",
      "iterations:  351\n",
      "iterations:  352\n",
      "iterations:  353\n",
      "iterations:  354\n",
      "iterations:  355\n",
      "iterations:  356\n",
      "iterations:  357\n",
      "iterations:  358\n",
      "iterations:  359\n",
      "iterations:  360\n",
      "current accuracy:  0.8630952380952381\n",
      "iterations:  361\n",
      "iterations:  362\n",
      "iterations:  363\n",
      "iterations:  364\n",
      "iterations:  365\n",
      "iterations:  366\n",
      "iterations:  367\n",
      "iterations:  368\n",
      "iterations:  369\n",
      "iterations:  370\n",
      "current accuracy:  0.8621428571428571\n",
      "iterations:  371\n",
      "iterations:  372\n",
      "iterations:  373\n",
      "iterations:  374\n",
      "iterations:  375\n",
      "iterations:  376\n",
      "iterations:  377\n",
      "iterations:  378\n",
      "iterations:  379\n",
      "iterations:  380\n",
      "current accuracy:  0.87\n",
      "iterations:  381\n",
      "iterations:  382\n",
      "iterations:  383\n",
      "iterations:  384\n",
      "iterations:  385\n",
      "iterations:  386\n",
      "iterations:  387\n",
      "iterations:  388\n",
      "iterations:  389\n",
      "iterations:  390\n",
      "current accuracy:  0.8685714285714285\n",
      "iterations:  391\n",
      "iterations:  392\n",
      "iterations:  393\n",
      "iterations:  394\n",
      "iterations:  395\n",
      "iterations:  396\n",
      "iterations:  397\n",
      "iterations:  398\n",
      "iterations:  399\n",
      "iterations:  400\n",
      "current accuracy:  0.8666666666666667\n",
      "iterations:  401\n",
      "iterations:  402\n",
      "iterations:  403\n",
      "iterations:  404\n",
      "iterations:  405\n",
      "iterations:  406\n",
      "iterations:  407\n",
      "iterations:  408\n",
      "iterations:  409\n",
      "iterations:  410\n",
      "current accuracy:  0.8785714285714286\n",
      "iterations:  411\n",
      "iterations:  412\n",
      "iterations:  413\n",
      "iterations:  414\n",
      "iterations:  415\n",
      "iterations:  416\n",
      "iterations:  417\n",
      "iterations:  418\n",
      "iterations:  419\n",
      "iterations:  420\n",
      "current accuracy:  0.865\n",
      "iterations:  421\n",
      "iterations:  422\n",
      "iterations:  423\n",
      "iterations:  424\n",
      "iterations:  425\n",
      "iterations:  426\n",
      "iterations:  427\n",
      "iterations:  428\n",
      "iterations:  429\n",
      "iterations:  430\n",
      "current accuracy:  0.8733333333333333\n",
      "iterations:  431\n",
      "iterations:  432\n",
      "iterations:  433\n",
      "iterations:  434\n",
      "iterations:  435\n",
      "iterations:  436\n",
      "iterations:  437\n",
      "iterations:  438\n",
      "iterations:  439\n",
      "iterations:  440\n",
      "current accuracy:  0.8714285714285714\n",
      "iterations:  441\n",
      "iterations:  442\n",
      "iterations:  443\n",
      "iterations:  444\n",
      "iterations:  445\n",
      "iterations:  446\n",
      "iterations:  447\n",
      "iterations:  448\n",
      "iterations:  449\n",
      "iterations:  450\n",
      "current accuracy:  0.8683333333333334\n",
      "iterations:  451\n",
      "iterations:  452\n",
      "iterations:  453\n",
      "iterations:  454\n",
      "iterations:  455\n",
      "iterations:  456\n",
      "iterations:  457\n",
      "iterations:  458\n",
      "iterations:  459\n",
      "iterations:  460\n",
      "current accuracy:  0.8807142857142857\n",
      "iterations:  461\n",
      "iterations:  462\n",
      "iterations:  463\n",
      "iterations:  464\n",
      "iterations:  465\n",
      "iterations:  466\n",
      "iterations:  467\n",
      "iterations:  468\n",
      "iterations:  469\n",
      "iterations:  470\n",
      "current accuracy:  0.8728571428571429\n",
      "iterations:  471\n",
      "iterations:  472\n",
      "iterations:  473\n",
      "iterations:  474\n",
      "iterations:  475\n",
      "iterations:  476\n",
      "iterations:  477\n",
      "iterations:  478\n",
      "iterations:  479\n",
      "iterations:  480\n",
      "current accuracy:  0.8759523809523809\n",
      "iterations:  481\n",
      "iterations:  482\n",
      "iterations:  483\n",
      "iterations:  484\n",
      "iterations:  485\n",
      "iterations:  486\n",
      "iterations:  487\n",
      "iterations:  488\n",
      "iterations:  489\n",
      "iterations:  490\n",
      "current accuracy:  0.8764285714285714\n",
      "iterations:  491\n",
      "iterations:  492\n",
      "iterations:  493\n",
      "iterations:  494\n",
      "iterations:  495\n",
      "iterations:  496\n",
      "iterations:  497\n",
      "iterations:  498\n",
      "iterations:  499\n",
      "iterations:  500\n",
      "current accuracy:  0.8757142857142857\n",
      "iterations:  501\n",
      "iterations:  502\n",
      "iterations:  503\n",
      "iterations:  504\n",
      "iterations:  505\n",
      "iterations:  506\n",
      "iterations:  507\n",
      "iterations:  508\n",
      "iterations:  509\n",
      "iterations:  510\n",
      "current accuracy:  0.8721428571428571\n",
      "iterations:  511\n",
      "iterations:  512\n",
      "iterations:  513\n",
      "iterations:  514\n",
      "iterations:  515\n",
      "iterations:  516\n",
      "iterations:  517\n",
      "iterations:  518\n",
      "iterations:  519\n",
      "iterations:  520\n",
      "current accuracy:  0.8730952380952381\n",
      "iterations:  521\n",
      "iterations:  522\n",
      "iterations:  523\n",
      "iterations:  524\n",
      "iterations:  525\n",
      "iterations:  526\n",
      "iterations:  527\n",
      "iterations:  528\n",
      "iterations:  529\n",
      "iterations:  530\n",
      "current accuracy:  0.8771428571428571\n",
      "iterations:  531\n",
      "iterations:  532\n",
      "iterations:  533\n",
      "iterations:  534\n",
      "iterations:  535\n",
      "iterations:  536\n",
      "iterations:  537\n",
      "iterations:  538\n",
      "iterations:  539\n",
      "iterations:  540\n",
      "current accuracy:  0.8826190476190476\n",
      "iterations:  541\n",
      "iterations:  542\n",
      "iterations:  543\n",
      "iterations:  544\n",
      "iterations:  545\n",
      "iterations:  546\n",
      "iterations:  547\n",
      "iterations:  548\n",
      "iterations:  549\n",
      "iterations:  550\n",
      "current accuracy:  0.8685714285714285\n",
      "iterations:  551\n",
      "iterations:  552\n",
      "iterations:  553\n",
      "iterations:  554\n",
      "iterations:  555\n",
      "iterations:  556\n",
      "iterations:  557\n",
      "iterations:  558\n",
      "iterations:  559\n",
      "iterations:  560\n",
      "current accuracy:  0.8747619047619047\n",
      "iterations:  561\n",
      "iterations:  562\n",
      "iterations:  563\n",
      "iterations:  564\n",
      "iterations:  565\n",
      "iterations:  566\n",
      "iterations:  567\n",
      "iterations:  568\n",
      "iterations:  569\n",
      "iterations:  570\n",
      "current accuracy:  0.8761904761904762\n",
      "iterations:  571\n",
      "iterations:  572\n",
      "iterations:  573\n",
      "iterations:  574\n",
      "iterations:  575\n",
      "iterations:  576\n",
      "iterations:  577\n",
      "iterations:  578\n",
      "iterations:  579\n",
      "iterations:  580\n",
      "current accuracy:  0.8852380952380953\n",
      "iterations:  581\n",
      "iterations:  582\n",
      "iterations:  583\n",
      "iterations:  584\n",
      "iterations:  585\n",
      "iterations:  586\n",
      "iterations:  587\n",
      "iterations:  588\n",
      "iterations:  589\n",
      "iterations:  590\n",
      "current accuracy:  0.8852380952380953\n",
      "iterations:  591\n",
      "iterations:  592\n",
      "iterations:  593\n",
      "iterations:  594\n",
      "iterations:  595\n",
      "iterations:  596\n",
      "iterations:  597\n",
      "iterations:  598\n",
      "iterations:  599\n",
      "iterations:  600\n",
      "current accuracy:  0.8802380952380953\n",
      "iterations:  601\n",
      "iterations:  602\n",
      "iterations:  603\n",
      "iterations:  604\n",
      "iterations:  605\n",
      "iterations:  606\n",
      "iterations:  607\n",
      "iterations:  608\n",
      "iterations:  609\n",
      "iterations:  610\n",
      "current accuracy:  0.8864285714285715\n",
      "iterations:  611\n",
      "iterations:  612\n",
      "iterations:  613\n",
      "iterations:  614\n",
      "iterations:  615\n",
      "iterations:  616\n",
      "iterations:  617\n",
      "iterations:  618\n",
      "iterations:  619\n",
      "iterations:  620\n",
      "current accuracy:  0.8747619047619047\n",
      "iterations:  621\n",
      "iterations:  622\n",
      "iterations:  623\n",
      "iterations:  624\n",
      "iterations:  625\n",
      "iterations:  626\n",
      "iterations:  627\n",
      "iterations:  628\n",
      "iterations:  629\n",
      "iterations:  630\n",
      "current accuracy:  0.8811904761904762\n",
      "iterations:  631\n",
      "iterations:  632\n",
      "iterations:  633\n",
      "iterations:  634\n",
      "iterations:  635\n",
      "iterations:  636\n",
      "iterations:  637\n",
      "iterations:  638\n",
      "iterations:  639\n",
      "iterations:  640\n",
      "current accuracy:  0.8769047619047619\n",
      "iterations:  641\n",
      "iterations:  642\n",
      "iterations:  643\n",
      "iterations:  644\n",
      "iterations:  645\n",
      "iterations:  646\n",
      "iterations:  647\n",
      "iterations:  648\n",
      "iterations:  649\n",
      "iterations:  650\n",
      "current accuracy:  0.8802380952380953\n",
      "iterations:  651\n",
      "iterations:  652\n",
      "iterations:  653\n",
      "iterations:  654\n",
      "iterations:  655\n",
      "iterations:  656\n",
      "iterations:  657\n",
      "iterations:  658\n",
      "iterations:  659\n",
      "iterations:  660\n",
      "current accuracy:  0.8826190476190476\n",
      "iterations:  661\n",
      "iterations:  662\n",
      "iterations:  663\n",
      "iterations:  664\n",
      "iterations:  665\n",
      "iterations:  666\n",
      "iterations:  667\n",
      "iterations:  668\n",
      "iterations:  669\n",
      "iterations:  670\n",
      "current accuracy:  0.8807142857142857\n",
      "iterations:  671\n",
      "iterations:  672\n",
      "iterations:  673\n",
      "iterations:  674\n",
      "iterations:  675\n",
      "iterations:  676\n",
      "iterations:  677\n",
      "iterations:  678\n",
      "iterations:  679\n",
      "iterations:  680\n",
      "current accuracy:  0.8792857142857142\n",
      "iterations:  681\n",
      "iterations:  682\n",
      "iterations:  683\n",
      "iterations:  684\n",
      "iterations:  685\n",
      "iterations:  686\n",
      "iterations:  687\n",
      "iterations:  688\n",
      "iterations:  689\n",
      "iterations:  690\n",
      "current accuracy:  0.8935714285714286\n",
      "iterations:  691\n",
      "iterations:  692\n",
      "iterations:  693\n",
      "iterations:  694\n",
      "iterations:  695\n",
      "iterations:  696\n",
      "iterations:  697\n",
      "iterations:  698\n",
      "iterations:  699\n",
      "iterations:  700\n",
      "current accuracy:  0.8904761904761904\n",
      "iterations:  701\n",
      "iterations:  702\n",
      "iterations:  703\n",
      "iterations:  704\n",
      "iterations:  705\n",
      "iterations:  706\n",
      "iterations:  707\n",
      "iterations:  708\n",
      "iterations:  709\n",
      "iterations:  710\n",
      "current accuracy:  0.8854761904761905\n",
      "iterations:  711\n",
      "iterations:  712\n",
      "iterations:  713\n",
      "iterations:  714\n",
      "iterations:  715\n",
      "iterations:  716\n",
      "iterations:  717\n",
      "iterations:  718\n",
      "iterations:  719\n",
      "iterations:  720\n",
      "current accuracy:  0.8864285714285715\n",
      "iterations:  721\n",
      "iterations:  722\n",
      "iterations:  723\n",
      "iterations:  724\n",
      "iterations:  725\n",
      "iterations:  726\n",
      "iterations:  727\n",
      "iterations:  728\n",
      "iterations:  729\n",
      "iterations:  730\n",
      "current accuracy:  0.8928571428571429\n",
      "iterations:  731\n",
      "iterations:  732\n",
      "iterations:  733\n",
      "iterations:  734\n",
      "iterations:  735\n",
      "iterations:  736\n",
      "iterations:  737\n",
      "iterations:  738\n",
      "iterations:  739\n",
      "iterations:  740\n",
      "current accuracy:  0.8945238095238095\n",
      "iterations:  741\n",
      "iterations:  742\n",
      "iterations:  743\n",
      "iterations:  744\n",
      "iterations:  745\n",
      "iterations:  746\n",
      "iterations:  747\n",
      "iterations:  748\n",
      "iterations:  749\n",
      "iterations:  750\n",
      "current accuracy:  0.8828571428571429\n",
      "iterations:  751\n",
      "iterations:  752\n",
      "iterations:  753\n",
      "iterations:  754\n",
      "iterations:  755\n",
      "iterations:  756\n",
      "iterations:  757\n",
      "iterations:  758\n",
      "iterations:  759\n",
      "iterations:  760\n",
      "current accuracy:  0.8933333333333333\n",
      "iterations:  761\n",
      "iterations:  762\n",
      "iterations:  763\n",
      "iterations:  764\n",
      "iterations:  765\n",
      "iterations:  766\n",
      "iterations:  767\n",
      "iterations:  768\n",
      "iterations:  769\n",
      "iterations:  770\n",
      "current accuracy:  0.8969047619047619\n",
      "iterations:  771\n",
      "iterations:  772\n",
      "iterations:  773\n",
      "iterations:  774\n",
      "iterations:  775\n",
      "iterations:  776\n",
      "iterations:  777\n",
      "iterations:  778\n",
      "iterations:  779\n",
      "iterations:  780\n",
      "current accuracy:  0.8883333333333333\n",
      "iterations:  781\n",
      "iterations:  782\n",
      "iterations:  783\n",
      "iterations:  784\n",
      "iterations:  785\n",
      "iterations:  786\n",
      "iterations:  787\n",
      "iterations:  788\n",
      "iterations:  789\n",
      "iterations:  790\n",
      "current accuracy:  0.8897619047619048\n",
      "iterations:  791\n",
      "iterations:  792\n",
      "iterations:  793\n",
      "iterations:  794\n",
      "iterations:  795\n",
      "iterations:  796\n",
      "iterations:  797\n",
      "iterations:  798\n",
      "iterations:  799\n",
      "iterations:  800\n",
      "current accuracy:  0.8890476190476191\n",
      "iterations:  801\n",
      "iterations:  802\n",
      "iterations:  803\n",
      "iterations:  804\n",
      "iterations:  805\n",
      "iterations:  806\n",
      "iterations:  807\n",
      "iterations:  808\n",
      "iterations:  809\n",
      "iterations:  810\n",
      "current accuracy:  0.89\n",
      "iterations:  811\n",
      "iterations:  812\n",
      "iterations:  813\n",
      "iterations:  814\n",
      "iterations:  815\n",
      "iterations:  816\n",
      "iterations:  817\n",
      "iterations:  818\n",
      "iterations:  819\n",
      "iterations:  820\n",
      "current accuracy:  0.8942857142857142\n",
      "iterations:  821\n",
      "iterations:  822\n",
      "iterations:  823\n",
      "iterations:  824\n",
      "iterations:  825\n",
      "iterations:  826\n",
      "iterations:  827\n",
      "iterations:  828\n",
      "iterations:  829\n",
      "iterations:  830\n",
      "current accuracy:  0.8904761904761904\n",
      "iterations:  831\n",
      "iterations:  832\n",
      "iterations:  833\n",
      "iterations:  834\n",
      "iterations:  835\n",
      "iterations:  836\n",
      "iterations:  837\n",
      "iterations:  838\n",
      "iterations:  839\n",
      "iterations:  840\n",
      "current accuracy:  0.8890476190476191\n",
      "iterations:  841\n",
      "iterations:  842\n",
      "iterations:  843\n",
      "iterations:  844\n",
      "iterations:  845\n",
      "iterations:  846\n",
      "iterations:  847\n",
      "iterations:  848\n",
      "iterations:  849\n",
      "iterations:  850\n",
      "current accuracy:  0.9\n",
      "iterations:  851\n",
      "iterations:  852\n",
      "iterations:  853\n",
      "iterations:  854\n",
      "iterations:  855\n",
      "iterations:  856\n",
      "iterations:  857\n",
      "iterations:  858\n",
      "iterations:  859\n",
      "iterations:  860\n",
      "current accuracy:  0.8983333333333333\n",
      "iterations:  861\n",
      "iterations:  862\n",
      "iterations:  863\n",
      "iterations:  864\n",
      "iterations:  865\n",
      "iterations:  866\n",
      "iterations:  867\n",
      "iterations:  868\n",
      "iterations:  869\n",
      "iterations:  870\n",
      "current accuracy:  0.9035714285714286\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0, 6, 7, ..., 4, 1, 2]), 0.8918333333333334)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mynn = ffnn([100, 70], batch_size = 0.1, learning_rate = 0.04)\n",
    "mynn.train(trainX, trainY.reshape(trainY.shape[0], 1))\n",
    "mynn.predict(testX, testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
